import pandas as pd
import numpy as np
from typing import List, Dict, Any
import faiss
from sentence_transformers import SentenceTransformer
import litellm
import json
import re
from config import MODEL

class RAGSystem:
    """
    ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
    ‡πÉ‡∏ä‡πâ FAISS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Vector ‡πÅ‡∏•‡∏∞ LiteLLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    """
    
    def __init__(self):
        """
        Initialize RAG System
        
        Args:
            model_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô LiteLLM
        """
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô embeddings
        self.encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î LLM model
        self.llm_model = MODEL
        
        # 3. ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.df = None
        self.documents = []
        self.embeddings = None
        self.index = None
        
    def load_excel_data(self, file_path: str) -> pd.DataFrame:
        """
        ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Excel
            
        Returns:
            DataFrame ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        """
        print("üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel...")
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel
        df = pd.read_excel(file_path)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏•‡∏ö space ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©)
        df.columns = df.columns.str.strip()
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ NaN ‡πÄ‡∏õ‡πá‡∏ô empty string
        df = df.fillna('')
        
        # ‡πÄ‡∏Å‡πá‡∏ö DataFrame
        self.df = df
        
        print(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤")
        print(f"üìä ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö: {df.columns.tolist()}")
        
        return df
    
    def prepare_documents(self) -> List[Dict]:
        """
        ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞ metadata
        
        Returns:
            List ‡∏Ç‡∏≠‡∏á documents ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
        """
        print("\nüìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...")
        
        documents = []
        
        for idx, row in self.df.iterrows():
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            text_parts = []
            
            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            if '‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤' in row and row['‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤']:
                text_parts.append(f"‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤: {row['‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤']}")
            
            if '‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤' in row and row['‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤']:
                text_parts.append(f"‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤: {row['‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤']}")
            
            if '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï' in row and row['‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']:
                text_parts.append(f"‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï: {row['‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']}")
            
            if '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏' in row and row['‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']:
                text_parts.append(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: {row['‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']}")
            
            if '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤' in row and row['‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤']:
                text_parts.append(f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {row['‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤']}")

            if '‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô' in row and row['‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô']:
                text_parts.append(f"‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô: {row['‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô']}")

            if '‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô' in row and row['‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô']:
                text_parts.append(f"‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô: {row['‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô']}")

            if '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤' in row and row['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤']:
                text_parts.append(f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤: {row['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤']}")

            if '‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' in row and row['‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤']:
                text_parts.append(f"‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤: {row['‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤']}")

            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            full_text = " ".join(text_parts)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á document dictionary
            doc = {
                'id': idx,
                'text': full_text,
                'metadata': row.to_dict()  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô metadata
            }
            
            documents.append(doc)
        
        self.documents = documents
        print(f"‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        
        return documents
    
    def create_embeddings(self) -> np.ndarray:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Returns:
            numpy array ‡∏Ç‡∏≠‡∏á embeddings
        """
        print("\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings...")
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å documents
        texts = [doc['text'] for doc in self.documents]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array ‡πÅ‡∏•‡∏∞ normalize
        embeddings = np.array(embeddings).astype('float32')
        
        # Normalize vectors for cosine similarity
        faiss.normalize_L2(embeddings)
        
        self.embeddings = embeddings
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: shape {embeddings.shape}")
        
        return embeddings
    
    def build_faiss_index(self):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö vector similarity
        """
        print("\nüèóÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index...")
        
        # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á embedding dimension
        dimension = self.embeddings.shape[1]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡πÅ‡∏ö‡∏ö Inner Product (‡∏´‡∏•‡∏±‡∏á normalize = cosine similarity)
        self.index = faiss.IndexFlatIP(dimension)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÄ‡∏Ç‡πâ‡∏≤ index
        self.index.add(self.embeddings)
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {self.index.ntotal} vectors")
    
    def extract_year_term(self, query: str):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏≠‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        ‡πÄ‡∏ä‡πà‡∏ô '‡∏õ‡∏µ 3 ‡πÄ‡∏ó‡∏≠‡∏° 1', '‡∏õ‡∏µ‡∏™‡∏≠‡∏á', '‡πÄ‡∏ó‡∏≠‡∏°‡∏™‡∏≠‡∏á', ‡∏Ø‡∏•‡∏Ø
        """
        year = None
        term = None
        plan = None

        # ‡∏à‡∏±‡∏ö‡∏õ‡∏µ
        year_match = re.search(r'‡∏õ‡∏µ\s*([1-4])', query)
        if year_match:
            year = int(year_match.group(1))

        # ‡∏à‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°
        term_match = re.search(r'‡πÄ‡∏ó‡∏≠‡∏°\s*([1-2])', query)
        if term_match:
            term = int(term_match.group(1))

        plan_match = re.search(r'(‡πÅ‡∏ú‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥|‡πÅ‡∏ú‡∏ô‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤|‡πÅ‡∏ú‡∏ô‡∏™‡∏´‡∏Å‡∏¥‡∏à)', query)
        if plan_match:
            plan = plan_match.group(1).strip()

        return year, term, plan

    def search(self, query: str, k: int = 5) -> List[Dict]:
        print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: '{query}'")

        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°
        year, term, plan = self.extract_year_term(query)

        filtered_docs = self.documents

        # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏≠‡∏° ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏á metadata ‡∏Å‡πà‡∏≠‡∏ô
        if any([year, term, plan]):
            def match_meta(doc):
                meta = doc['metadata']
                conds = []
                if year:
                    conds.append(str(year) in str(meta.get('‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '')).strip())
                if term:
                    conds.append(str(term) in str(meta.get('‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô', '')).strip())
                if plan:
                    conds.append(plan in str(meta.get('‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '')).strip())
                return all(conds) if conds else True
            filtered_docs = [d for d in self.documents if match_meta(d)]
            print(f"üìö ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(filtered_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏ï‡∏≤‡∏°‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°")

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡πÉ‡∏´‡πâ fallback ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            if not filtered_docs:
                filtered_docs = self.documents

        # ‡πÅ‡∏õ‡∏•‡∏á query ‡πÄ‡∏õ‡πá‡∏ô embedding
        query_embedding = self.encoder.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        texts = [d['text'] for d in filtered_docs]
        doc_embeddings = self.encoder.encode(texts)
        doc_embeddings = np.array(doc_embeddings).astype('float32')
        faiss.normalize_L2(doc_embeddings)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á index ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        temp_index = faiss.IndexFlatIP(doc_embeddings.shape[1])
        temp_index.add(doc_embeddings)

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        distances, indices = temp_index.search(query_embedding, k)

        results = []
        for i, idx in enumerate(indices[0]):
            if idx >= 0:
                results.append({
                    'document': filtered_docs[idx],
                    'score': float(distances[0][i])
                })

        print(f"‚úÖ ‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return results

    
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ LLM ‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            context_docs: ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            
        Returns:
            ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ LLM
        """
        print("\nüí≠ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            metadata = doc['document']['metadata']
            context_parts.append(f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {i}:")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å metadata
            for key, value in metadata.items():
                if value and str(value).strip():  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
                    context_parts.append(f"- {key}: {value}")
            
            context_parts.append("")  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á
        
        context = "\n".join(context_parts)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ 
        ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤:
        {context}

        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

        ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢):"""
        
        try:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏ú‡πà‡∏≤‡∏ô LiteLLM
            response = litellm.completion(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return answer
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {str(e)}"
    
    def query(self, question: str, k: int = 5) -> Dict[str, Any]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö
        
        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
            
        Returns:
            Dictionary ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
        """
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        search_results = self.search(question, k)
        
        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
        answer = self.generate_answer(question, search_results)
        
        # 3. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result = {
            'question': question,
            'answer': answer,
            'sources': search_results
        }
        
        return result
    
    def save_index(self, path: str):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS index"""
        faiss.write_index(self.index, path)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å index ‡∏ó‡∏µ‡πà: {path}")
    
    def load_index(self, path: str):
        """‡πÇ‡∏´‡∏•‡∏î FAISS index"""
        self.index = faiss.read_index(path)
        print(f"üìÇ ‡πÇ‡∏´‡∏•‡∏î index ‡∏à‡∏≤‡∏Å: {path}")


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
def main():
    """
    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG
    """
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö RAG
    rag = RAGSystem()
    
    # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà path ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢ path ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Excel
    df = rag.load_excel_data("cs_coursedata.xlsx")
    
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    rag.prepare_documents()
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
    rag.create_embeddings()
    
    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index
    rag.build_faiss_index()
    
    # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å index (optional)
    rag.save_index("course_index.faiss")
    
    # 7. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    questions = [
        "‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?",
        "‡∏ß‡∏¥‡∏ä‡∏≤ Database ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï?",
        "‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤ Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
        "‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏•‡∏á‡πÉ‡∏ô‡∏ñ‡πâ‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ ‡∏õ‡∏µ 3 ‡πÄ‡∏ó‡∏≠‡∏° 1"
    ]
    
    for q in questions:
        print(f"\n{'='*60}")
        result = rag.query(q, k=5)
        print(f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {result['question']}")
        print(f"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {result['answer']}")
        print(f"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: ‡∏û‡∏ö {len(result['sources'])} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")


if __name__ == "__main__":
    main()