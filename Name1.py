import os
import streamlit as st
import pandas as pd
import base64
import json
import time
from pathlib import Path

from utils.llm_client import LLMClient, get_available_models

from rag_core import (
    ensure_index, hybrid_search, filter_df, parse_year_term_plan,
    extract_interests, recommend_courses, build_context, format_course_row
)

from config import MODEL
from litellm import completion

EXCEL_PATH = "/mnt/c/Users/petez/OneDrive/Documents/dawg_sheet/cs_coursedata.xlsx"
st.set_page_config(page_title="Study Helper (RAG Chat)",page_icon="üìö",layout="wide")

#image
def get_base64_image(image_file):
    if not os.path.exists(image_file):
        return ""
        
    try:
        with open(image_file, "rb") as f:
            return base64.b64encode(f.read()).decode()
    except Exception:
        return ""
    
def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "llm_client" not in st.session_state:
        st.session_state.llm_client = None
    if "rag_system" not in st.session_state:
        st.session_state.rag_system = None
    if "rag_initialized" not in st.session_state:
        st.session_state.rag_initialized = False
    if "is_model_ready" not in st.session_state:
        st.session_state.is_model_ready = False
    if "is_rag_ready" not in st.session_state:
        st.session_state.is_rag_ready = False
# Main
def main():
    init_session_state()
    
    #background
    current_dir = os.path.dirname(__file__)
    image_path = os.path.join(current_dir, "png", "background1.png")
    base64_encoded = get_base64_image(image_path)
    if base64_encoded:
        css = f"""
        <style>
        .stApp {{
            background-image: url("data:image/jpeg;base64,{base64_encoded}");
            background-position: center; 
            background-repeat: no-repeat;
            background-attachment: fixed;
            background-size: cover; 
            background-color: #000000;
            background-image: none !important;
        }}
        </style>
        """
        st.markdown(css, unsafe_allow_html=True)
    
    header_col1, header_col2, header_col3 = st.columns([1, 10, 1])
    
    #title center
    title_color = "#000000"
    
    with header_col2:
        st.markdown(f"""
        <div style="text-align: center;">
            <h1 style="color: {title_color};">üìö Study Helper</h1>
            <p>This is a RAG-powered Study Helper for CS CMU projects.</p>
        </div>
        """, unsafe_allow_html=True)

    with st.sidebar:
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            # ‚úÖ ‡πÉ‡∏ä‡πâ path ‡∏ó‡∏µ‡πà‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå app3.py
            current_dir = os.path.dirname(__file__)
            logo_path = os.path.join(current_dir, "png", "cslogo2.png")

            if os.path.exists(logo_path):
                st.image(logo_path, width=100)
            else:
                st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏•‡πÇ‡∏Å‡πâ: {logo_path}")

if __name__ == "__main__":
    main()


@st.cache_data(show_spinner=False)
def load_data(excel_path: str):
    df = pd.read_excel(excel_path)
    # Keep only the known columns (as in the file)
    cols = ['‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤','‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤','‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï','‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏','‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤','‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô','‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô','‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤','‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤']
    df = df[[c for c in cols if c in df.columns]].copy()
    # Normalize types for robust filtering
    for c in ["‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤","‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤","‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏","‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤","‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤","‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"]:
        if c in df.columns: df[c] = df[c].astype(str)
    for c in ["‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô","‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï"]:
        if c in df.columns: df[c] = df[c].astype(str)
    return df

@st.cache_resource(show_spinner=True)
def get_index_bundle(df, excel_path: str):
    # cache_dir in the app folder
    cache_dir = os.path.join(os.path.dirname(excel_path), ".faiss_cache")
    bundle = ensure_index(df, excel_path, cache_dir=cache_dir)
    # we don't cache the st_model to keep memory light; ensure_index returns cached on disk
    return bundle

def call_llm(system_prompt: str, user_prompt: str, model_name: str, max_tokens: int = 600):
    resp = completion(
        model=model_name,
        messages=[
            {"role":"system","content":system_prompt},
            {"role":"user","content":user_prompt}
        ],
        temperature=temperature,
        max_tokens=max_tokens
    )
    try:
        return resp["choices"][0]["message"]["content"]
    except Exception:
        return str(resp)

def system_prompt_th():
    return (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏Ñ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (context) ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
        "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ñ‡∏ß‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ç‡∏≤‡∏î‡∏ï‡∏Å‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"
    )

def build_user_prompt(question: str, context: str):
    guide = (
        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:\n\n"
        "### ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó\n"
        f"{context}\n\n"
        "### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
        f"{question}\n\n"
        "‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ñ‡∏≤‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï/‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö/‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó:\n"
        "- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï'\n"
        "- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö/‡πÄ‡∏•‡∏∑‡∏≠‡∏Å): ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤'\n"
        "- ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤'\n"
        "- ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏π‡πâ: '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏'\n"
    )
    return guide

def system_prompt_recommend_th():
    return (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏Ñ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå "
        "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ "
        "‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏´‡∏°‡πà "
        "‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ‡∏°‡∏≤ (‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô) "
        "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏à‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"
    )

def apply_filters(df, year, term, plan, course_type):
    year_v = None if year == "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" else year
    term_v = None if term == "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" else term
    plan_v = None if plan == "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" else plan
    tmp = filter_df(df, year_v, term_v, plan_v)
    if course_type != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" and "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤" in tmp.columns:
        if course_type == "‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö":
            tmp = tmp[tmp["‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤"].astype(str).str.contains("‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö|‡πÅ‡∏Å‡∏ô", na=False)]
        elif course_type == "‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å":
            tmp = tmp[tmp["‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤"].astype(str).str.contains("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", na=False)]

    return tmp

def answer_general(question: str):
    # Hybrid search on filtered df
    hits = hybrid_search(question, df_filtered, bundle, k=topk, alpha=alpha)
    if not hits:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ß‡πâ"

    context = build_context(df_filtered, hits)
    sys = system_prompt_th()
    user = build_user_prompt(question, context)
    return call_llm(sys, user, model_name=selected_model, max_tokens=max_tokens)

def answer_recommend(question: str, num_of_course: int = 5):
    info = parse_year_term_plan(question)
    tmp = df
    # Enforce filter as spec: filter BEFORE RAG/recommend
    tmp = apply_filters(tmp,
                        info["year"] if info["year"] else year,
                        info["term"] if info["term"] else term,
                        info["plan"] if info["plan"] else plan,
                        course_type)
    if tmp.empty:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á"

    interests = extract_interests(question)
    recs = recommend_courses(tmp, interests, limit=8)

    lines = []
    if interests:
        lines.append(f"**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ:** {', '.join(interests)}")
    lines.append("**‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**")
    course_counter = 0
    for _, row in recs.iterrows():
        if course_counter < num_of_course:
            bullet = f"- {row['‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤']} {row['‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤']}\n‚Ä¢ {row['‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï | {row['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤']} | ‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô : ‡πÄ‡∏ó‡∏≠‡∏° {row['‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô']} | ‡πÅ‡∏ú‡∏ô: {row['‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤']}"
            if str(row.get("‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤","")).strip():
                bullet += f"\n  ‚Ä¢ ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤: {row['‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤']}"
            if str(row.get("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏","")).strip():
                bullet += f"\n  ‚Ä¢ ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: {row['‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']}"
            lines.append(bullet)
            course_counter += 1
        else:
            break
    lines.append("\n*‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏ä‡πà‡∏ô '‡πÄ‡∏≠‡∏≤ 5 ‡∏ß‡∏¥‡∏ä‡∏≤' ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à*")
    return "\n".join(lines)

def answer_recommend_llm(question: str):
    # 1) parse ‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°/‡πÅ‡∏ú‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    info = parse_year_term_plan(question)

    # 2) ‡∏Å‡∏£‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô (‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å UI ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°)
    tmp = apply_filters(
        df,
        info["year"] if info["year"] else year,
        info["term"] if info["term"] else term,
        info["plan"] if info["plan"] else plan,
        course_type
    )
    if tmp.empty:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏£‡∏≠‡∏á"

    # 3) ‡∏î‡∏∂‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö
    interests = extract_interests(question)
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    recs = recommend_courses(tmp, interests, limit=12)

    # (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏ú‡∏™‡∏°‡∏ú‡∏• hybrid_search ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    # hits = hybrid_search(" ".join(interests) or question, tmp, bundle, k=5, alpha=alpha)
    # ctx_hits = build_context(tmp, hits)

    # 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ recs ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
    lines = []
    for _, row in recs.iterrows():
        lines.append(format_course_row(row))
    context = "\n\n---\n\n".join(lines)
    # context = context + ("\n\n" + ctx_hits if ctx_hits else "")  # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°

    # 5) Prompt ‡πÄ‡∏Ç‡πâ‡∏≤ LLM (‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô Context)
    sys = system_prompt_recommend_th()
    guide = (
        "‡∏à‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô Context "
        "(‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) "
        "‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Context "
        "‡∏´‡∏≤‡∏Å Context ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏' ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ.\n"
        f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ: {', '.join(interests) if interests else '‚Äî'}"
    )
    user = (
        f"{guide}\n\n### Context\n{context}\n\n"
        f"### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ\n{question}\n\n"
        "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ 1-2 ‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ bullet ‡∏™‡∏£‡∏∏‡∏õ (1) ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö (2) ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    )

    return call_llm(sys, user, model_name=selected_model, max_tokens=max_tokens)


def is_recommend_intent(text: str):
    keys = ["‡∏à‡∏±‡∏î","‡∏ï‡∏≠‡∏ô","‡∏ß‡∏¥‡∏ä‡∏≤","‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤","‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤","‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏∞‡πÑ‡∏£","‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏∞‡πÑ‡∏£","‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£","‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£","‡∏ä‡πà‡∏ß‡∏¢","‡∏Ñ‡∏ß‡∏£‡∏•‡∏á","‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô","‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô","‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î","‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö","‡πÄ‡∏•‡∏∑‡∏≠‡∏Å","‡πÅ‡∏ú‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥","‡πÅ‡∏ú‡∏ô‡∏™‡∏´‡∏Å‡∏¥‡∏à","‡πÅ‡∏ú‡∏ô‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤","‡∏õ‡∏Å‡∏ï‡∏¥","‡∏™‡∏´‡∏Å‡∏¥‡∏à","‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤"]
    return any(k in text for k in keys)

def toggle_mandatory():
    st.session_state.show_mandatory = not st.session_state.show_mandatory


# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")

    # Filter UI And Clear Chat
    with st.expander("‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", expanded=False):
        year = st.selectbox("‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", ("‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "1", "2", "3", "4"))
        term = st.selectbox("‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô", ("‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "1", "2"))
        plan = st.selectbox("‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", ("‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡πÅ‡∏ú‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥", "‡πÅ‡∏ú‡∏ô‡∏™‡∏´‡∏Å‡∏¥‡∏à", "‡πÅ‡∏ú‡∏ô‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤"))
        course_type = st.selectbox("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤", ("‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"))

    # ‡∏õ‡∏£‡∏±‡∏öRAG
    with st.expander("‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", expanded=False):
        temperature = st.slider("Temperature",min_value=0.1,max_value=1.0,value=0.4,step=0.1,help="‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°")
        max_tokens = st.slider("Max Tokens", min_value=500, max_value=2500, value=1500, step=100,help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Token ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ")
        alpha = st.slider("‡∏Ñ‡πà‡∏≤ Alpha", min_value=0.0, max_value=1.0, value=0.6, step=0.05,help="Alpha ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 1 ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Vector ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 0 ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Keyword")
        topk = st.slider("‡∏Ñ‡πà‡∏≤ k", min_value=5, max_value=15, value=10, step=1,help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
    st.divider()
    available_models = get_available_models()
    selected_model = st.selectbox(
            "Select Model",
            available_models,
            index=1,
            help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"
        )

    with st.spinner("Initializing model..."):
            st.session_state.llm_client = LLMClient(
                model=selected_model,
                temperature=temperature,
                max_tokens=max_tokens)
    # ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á
    st.info(f"‚ö°Model {selected_model} initialized!")
    # st.markdown(selected_model)
    use_llm_rec = st.checkbox("‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (RAG)", value=True)
    
    st.divider()
    #Clear Chat
    if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó"):
        st.session_state.messages = []
        st.rerun()
# Load data + index
df = load_data(EXCEL_PATH)
bundle = get_index_bundle(df, EXCEL_PATH)

df_filtered = apply_filters(df, year, term, plan, course_type)

# Chat state
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role":"assistant","content":"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ! ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô \"‡∏õ‡∏µ 2 ‡πÄ‡∏ó‡∏≠‡∏° 2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á\" ‡∏´‡∏£‡∏∑‡∏≠ \"‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏µ 3 ‡πÄ‡∏ó‡∏≠‡∏° 1 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI\""}
    ]

# Chat history UI
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
q = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if q:
    st.session_state.messages.append({"role":"user","content":q})
    with st.chat_message("user"):
        st.markdown(q)
    with st.chat_message("assistant"):
        with st.spinner("Processing..."):
            if is_recommend_intent(q):
                ans = answer_recommend_llm(q) if use_llm_rec else answer_recommend(q)
            else:
                ans = answer_general(q) 
            st.markdown(ans)
            st.session_state.messages.append({"role":"assistant","content":ans})

if "show_mandatory" not in st.session_state:
    st.session_state.show_mandatory = False


# ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡∏£‡∏≠‡∏á
st.markdown(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á:** {len(df_filtered)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
with st.expander("‡∏î‡∏π‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á"):
    showed_cols = ["‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤", "‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤", "‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï", "‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô"]
    showed_cols = [c for c in showed_cols if c in df_filtered.columns]
    st.dataframe(df_filtered[showed_cols].reset_index(drop=True))


# Example
cA, cB, cC = st.columns(3)
with cA:
    button_A = st.button("‡∏õ‡∏µ1 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á")

with cB:
    button_B =  st.button("‡∏ß‡∏¥‡∏ä‡∏≤ 204271 ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á")

with cC:
    st.download_button(
        "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô CSV",
        data=df_filtered.to_csv(index=False).encode("utf-8-sig"),
        file_name="filtered_courses.csv",
        mime="text/csv"
        )
if button_A:
    q = "‡∏õ‡∏µ1 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"
    st.session_state.messages.append({"role":"user","content":q})
    with st.chat_message("user"):
        st.markdown(q)
    with st.chat_message("assistant"):
        if is_recommend_intent(q):
            ans = answer_recommend_llm(q) if use_llm_rec else answer_recommend(q)
        else:
            ans = answer_general(q)
        st.markdown(ans)
        st.session_state.messages.append({"role":"assistant","content":ans})

if button_B:
    q = "‡∏ß‡∏¥‡∏ä‡∏≤ 204271 ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"
    st.session_state.messages.append({"role":"user","content":q})
    with st.chat_message("user"):
        st.markdown(q)
    with st.chat_message("assistant"):
        if is_recommend_intent(q):
            ans = answer_recommend_llm(q) if use_llm_rec else answer_recommend(q)
        else:
            ans = answer_general(q)
        st.markdown(ans)
        st.session_state.messages.append({"role":"assistant","content":ans})